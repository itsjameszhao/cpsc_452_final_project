{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Learning_Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIV4DHffpOXMpNZTAXWkur"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This Google Colab is for prototyping and developing our deep learning project. It's linked to the Github repo that Yoony made so updating the code in this repo and `git push`ing will propagate any changes to Github. The convenient thing about it is that we don't have to install any packages (Google handles that for us) so we can start writing code right away. We also don't have to download and upload training data multiple times since we will be able to directly access the training data we have in Google Drive. We can also use this notebook to preprocess any data we have. \n","\n"],"metadata":{"id":"szHJTPNL-_A7"}},{"cell_type":"markdown","source":["# Import Required Libraries"],"metadata":{"id":"oKFSnokp3hZs"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"UNG3QwBRs4qf","executionInfo":{"status":"ok","timestamp":1649017561062,"user_tz":240,"elapsed":2599,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","import datetime\n","import numpy as np\n","import tensorflow\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import MaxPooling2D"]},{"cell_type":"markdown","source":["# Mount Drive, Change to Current Directory"],"metadata":{"id":"FvjwohX_3pdc"}},{"cell_type":"code","source":["# Make files from Google Drive viewable through Colab.\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMeg60AGufcV","executionInfo":{"status":"ok","timestamp":1649017583628,"user_tz":240,"elapsed":20442,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"2dfc7473-3b48-4940-93a6-9312bbc00bd0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# Change to the shared project folder\n","%cd 'drive/MyDrive/CPSC 452 Deep Learning Final Project/cpsc452-project'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2W64FIBQ56bS","executionInfo":{"status":"ok","timestamp":1649017586232,"user_tz":240,"elapsed":477,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"67977ab1-c9e7-4f37-a990-9ea603fa5351"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1vFHGYIR4br7lD84U7pCjnR8Q5HS0X-tu/CPSC 452 Deep Learning Final Project/cpsc452-project\n"]}]},{"cell_type":"code","source":["# Note: If it doesn't work for you remember to add a shortcut to the Meili's shared folder to your drive as detailed here:\n","# https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab"],"metadata":{"id":"zKgQ_WHvxOE3","executionInfo":{"status":"ok","timestamp":1649011940457,"user_tz":240,"elapsed":3,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Now we are in the Github repository!\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0QpTnz_9AkF","executionInfo":{"status":"ok","timestamp":1649017589889,"user_tz":240,"elapsed":414,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"39fd36f9-5031-43c8-fcad-599b8752f155"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":[" data\n","'Deep Learning Final Project Data Visualization.ipynb'\n"," Deep_Learning_Training.ipynb\n"," get_race_feature\n"," logs\n"," race_feature.csv\n"," README.md\n"," Scaling.ipynb\n"," train_face_detection.py\n"]}]},{"cell_type":"markdown","source":["# Github Integration Section (Only Need to Do Once!)"],"metadata":{"id":"lXev8ypP32M-"}},{"cell_type":"code","source":["# I followed this tutorial to get Github to integrate with Google Drive: \n","# https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d\n","\n","# GITHUB_TOKEN = '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'"],"metadata":{"id":"ywwgF_NN4Abf","executionInfo":{"status":"ok","timestamp":1649014499356,"user_tz":240,"elapsed":148,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Clone the repo into the \"CPSC 452 Deep Learning Final Project\" shared folder\n","# ONLY RUN ONCE: git clone https://{GITHUB_TOKEN}@github.com/ykim321/cpsc452-project.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGDzSvB64Ae-","executionInfo":{"status":"ok","timestamp":1649014522786,"user_tz":240,"elapsed":22619,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"557b3ae1-8d01-4a7a-8472-4e9ea753e338"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'cpsc452-project'...\n","remote: Enumerating objects: 139, done.\u001b[K\n","remote: Counting objects: 100% (139/139), done.\u001b[K\n","remote: Compressing objects: 100% (134/134), done.\u001b[K\n","remote: Total 139 (delta 5), reused 130 (delta 2), pack-reused 0\u001b[K\n","Receiving objects: 100% (139/139), 311.81 MiB | 15.38 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n"]}]},{"cell_type":"code","source":["# !git config user.email \"j.d.zhao@yale.edu\"\n","# !git config user.name \"James Zhao\""],"metadata":{"id":"xSU1l67R-gwu","executionInfo":{"status":"ok","timestamp":1649015447072,"user_tz":240,"elapsed":126,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Git Operations for Pushing / Pulling / Etc (Can Do Multiple Times)"],"metadata":{"id":"QWz5PV6f7lN_"}},{"cell_type":"code","source":["# get repo status\n","!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rO82UQuu-D7h","executionInfo":{"status":"ok","timestamp":1649017603929,"user_tz":240,"elapsed":7795,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"2da1be6e-44de-43c4-c954-b0c6f949010d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git checkout -- <file>...\" to discard changes in working directory)\n","\n","\t\u001b[31mmodified:   Deep_Learning_Training.ipynb\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mlogs/\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["# pull latest repo data\n","!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEU9wfkUutUJ","executionInfo":{"status":"ok","timestamp":1649016455305,"user_tz":240,"elapsed":618,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"6b3303f4-3061-48eb-f7fb-acf570445b9f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"code","source":["# add updated files \n","!git add -u"],"metadata":{"id":"EBM6vqjd9c11","executionInfo":{"status":"ok","timestamp":1649016492904,"user_tz":240,"elapsed":538,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# commit changes to remote\n","!git commit -m \"Added ipynb notebook\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fblvr0jt9xnd","executionInfo":{"status":"ok","timestamp":1649016504599,"user_tz":240,"elapsed":349,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"9c0dae4d-30d2-47c9-9802-c971a64eb773"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 555acaf] Added ipynb notebook\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 Deep_Learning_Training.ipynb\n"]}]},{"cell_type":"code","source":["# push to remote\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mqAwG5Mx4Hj","executionInfo":{"status":"ok","timestamp":1649016508787,"user_tz":240,"elapsed":948,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"fb374f1b-8e58-4594-88cb-ad6ae03eb72c"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Counting objects: 3, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  33% (1/3)   \rCompressing objects:  66% (2/3)   \rCompressing objects: 100% (3/3)   \rCompressing objects: 100% (3/3), done.\n","Writing objects:  33% (1/3)   \rWriting objects:  66% (2/3)   \rWriting objects: 100% (3/3)   \rWriting objects: 100% (3/3), 5.80 KiB | 1.93 MiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0)\n","remote: Resolving deltas:   0% (0/1)\u001b[K\rremote: Resolving deltas: 100% (1/1)\u001b[K\rremote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To https://github.com/ykim321/cpsc452-project.git\n","   045632c..555acaf  main -> main\n"]}]},{"cell_type":"markdown","source":["# Code for training the CNN face detection model"],"metadata":{"id":"D49iip-eDQcY"}},{"cell_type":"code","source":["# This file contains the training data\n","def load_data_from_npz_file(file_path):\n","    \"\"\"\n","    Load data from npz file\n","    :param file_path: path to npz file with training data\n","    :return: input features and target data as numpy arrays\n","    \"\"\"\n","    data = np.load(file_path)\n","    return data['input'], data['target']\n","\n","# Mean and variance centering data\n","def normalize_data_per_row(data):\n","    \"\"\"\n","    Normalize a give matrix of data (samples must be organized per row)\n","    :param data: input data as a numpy array with dimensions NxHxWxC\n","    :return: normalized data with pixel values in [0,1] (array with same dimensions as input)\n","    \"\"\"\n","\n","    # sanity checks!\n","    assert len(data.shape) == 4, \"Expected the input data to be a 4D matrix\"\n","\n","    return data / 255\n","\n","# Split into train and test \n","def split_data(input, target, train_percent):\n","    \"\"\"\n","    Split the input and target data into two sets\n","    :param input: inputs [NxM] matrix\n","    :param target: target [Nx1] matrix\n","    :param train_percent: percentage of the data that should be assigned to training\n","    :return: train_input, train_target, test_input, test_target\n","    \"\"\"\n","    assert input.shape[0] == target.shape[0], \\\n","        \"Number of inputs and targets do not match ({} vs {})\".format(input.shape[0], target.shape[0])\n","\n","    indices = list(range(input.shape[0]))\n","    np.random.shuffle(indices)\n","\n","    num_train = int(input.shape[0]*train_percent)\n","    train_indices = indices[:num_train]\n","    val_indices = indices[num_train:]\n","\n","    return input[train_indices, :], target[train_indices,:], input[val_indices,:], target[val_indices,:]\n","\n","# Build the model\n","def build_nonlinear_model():\n","    \"\"\"\n","    Build NN model with Keras\n","    :param num_inputs: number of input features for the model\n","    :return: Keras model\n","    \"\"\"\n","    \n","    model = Sequential()\n","    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(64,64,3)))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Flatten())\n","    model.add(Dense(32, activation=\"relu\"))\n","    model.add(Dense(1,activation=\"sigmoid\"))\n","    model.summary()\n","    \n","    return model\n","\n","def train_model(model, train_input, train_target, val_input, val_target,\n","                epochs=200, learning_rate=0.01, batch_size=16):\n","    \"\"\"\n","    Train the model on the given data\n","    :param model: Keras model\n","    :param train_input: train inputs\n","    :param train_target: train targets\n","    :param val_input: validation inputs\n","    :param val_target: validation targets\n","    :param input_mean: mean for the variables in the inputs (for normalization)\n","    :param input_stdev: st. dev. for the variables in the inputs (for normalization)\n","    :param epochs: epochs for gradient descent\n","    :param learning_rate: learning rate for gradient descent\n","    :param batch_size: batch size for training with gradient descent\n","    \"\"\"\n","\n","    norm_train_input = n2(train_input)\n","    norm_val_input = n2(val_input)\n","\n","    # compile the model: define optimizer, loss, and metrics\n","    model.compile(optimizer=tensorflow.keras.optimizers.Adam(lr=learning_rate),\n","                 loss='binary_crossentropy',\n","                 metrics=['binary_accuracy'])\n","                 \n","     # tensorboard callback\n","    logs_dir = 'logs/log_{}'.format(datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M\"))\n","    tbCallBack = tensorflow.keras.callbacks.TensorBoard(log_dir=logs_dir, write_graph=True)\n","\n","     # save checkpoint callback\n","    checkpointCallBack = tensorflow.keras.callbacks.ModelCheckpoint(os.path.join(logs_dir,'best_weights.h5'),\n","                                                            monitor='binary_accuracy',\n","                                                            verbose=0,\n","                                                            save_best_only=True,\n","                                                            save_weights_only=False,\n","                                                            mode='auto',\n","                                                            save_freq=1)\n","\n","    # do training for the specified number of epochs and with the given batch size\n","    # TODO - Add callbacks to fit function\n","    model.fit(norm_train_input, train_target, epochs=epochs, batch_size=batch_size,\n","          validation_data=(norm_val_input, val_target),\n","          callbacks=[tbCallBack, checkpointCallBack])"],"metadata":{"id":"ezh2t1KMs8Zx","executionInfo":{"status":"ok","timestamp":1649017256853,"user_tz":240,"elapsed":404,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def main(npz_data_file, batch_size, epochs, lr, val, logs_dir, build_fn=build_nonlinear_model):\n","    \"\"\"\n","    Main function that performs training and test on a validation set\n","    :param npz_data_file: npz input file with training data\n","    :param batch_size: batch size to use at training time\n","    :param epochs: number of epochs to train for\n","    :param lr: learning rate\n","    :param val: percentage of the training data to use as validation\n","    :param logs_dir: directory where to save logs and trained parameters/weights\n","    \"\"\"\n","\n","    input, target = load_data_from_npz_file(npz_data_file)\n","    N = input.shape[0]\n","    assert N == target.shape[0], \\\n","        \"The input and target arrays had different amounts of data ({} vs {})\".format(N, target.shape[0]) # sanity check!\n","    print(\"Loaded {} training examples.\".format(N))\n","\n","    train_input, train_target, val_input, val_target = split_data(input, target, val)\n","    model = build_fn()\n","    train_model(model, train_input, train_target, val_input, val_target, epochs=epochs, learning_rate=lr, batch_size=batch_size)\n","\n","def n2(data):\n","    \"\"\"\n","    Normalize a give matrix of data (samples must be organized per row)\n","    :param data: input data as a numpy array with dimensions NxHxWxC\n","    :return: normalized data with pixel values in [0,1] (array with same dimensions as input)\n","    \"\"\"\n","\n","    # sanity checks!\n","    assert len(data.shape) == 4, \"Expected the input data to be a 4D matrix\"\n","\n","    if np.max(data) > 255:\n","        normalized_data = data / 255\n","    else:\n","        normalized_data = data\n","    return normalized_data\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"ev0X1VK3tO9S","executionInfo":{"status":"error","timestamp":1649017553073,"user_tz":240,"elapsed":361,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"9273b025-bfd4-4d55-9918-33ddfbd8fe2c"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f030f9827fba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_data_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_nonlinear_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mMain\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mperforms\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtest\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mnpz_data_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpz\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0msize\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0mat\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'build_nonlinear_model' is not defined"]}]},{"cell_type":"code","source":["main(\"./data/64x64_data.npz\", 64, 100, 0.01, 0.1, \"./data/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":168},"id":"Aq8R9Oq5tlFY","executionInfo":{"status":"error","timestamp":1649017543032,"user_tz":240,"elapsed":510,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"938a60e7-965a-449e-ad02-1e33b06d106d"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7e58c836d88a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/64x64_data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lpmlXLWJtYye"},"execution_count":null,"outputs":[]}]}