{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Learning_Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMaROExTATpQOO/FLrRZ1p5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This Google Colab is for prototyping and developing our deep learning project. It's linked to the Github repo that Yoony made so updating the code in this repo and `git push`ing will propagate any changes to Github. The convenient thing about it is that we don't have to install any packages (Google handles that for us) so we can start writing code right away. We also don't have to download and upload training data multiple times since we will be able to directly access the training data we have in Google Drive. We can also use this notebook to preprocess any data we have. \n","\n"],"metadata":{"id":"szHJTPNL-_A7"}},{"cell_type":"markdown","source":["# Import Required Libraries"],"metadata":{"id":"oKFSnokp3hZs"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"UNG3QwBRs4qf","executionInfo":{"status":"ok","timestamp":1649014947552,"user_tz":240,"elapsed":6315,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"outputs":[],"source":["import os\n","import sys\n","import argparse\n","import datetime\n","import numpy as np\n","\n","import tensorflow\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import MaxPooling2D"]},{"cell_type":"markdown","source":["# Mount Drive, Change to Current Directory"],"metadata":{"id":"FvjwohX_3pdc"}},{"cell_type":"code","source":["# Make files from Google Drive viewable through Colab.\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMeg60AGufcV","executionInfo":{"status":"ok","timestamp":1649014953232,"user_tz":240,"elapsed":1106,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"e9343119-7a2e-4734-f31a-ae1f46c6fea9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Change to the shared project folder\n","%cd 'drive/MyDrive/CPSC 452 Deep Learning Final Project/cpsc452-project'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2W64FIBQ56bS","executionInfo":{"status":"ok","timestamp":1649014982039,"user_tz":240,"elapsed":120,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"75d18114-c9a4-4589-f155-6b8c92034f08"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1vFHGYIR4br7lD84U7pCjnR8Q5HS0X-tu/CPSC 452 Deep Learning Final Project/cpsc452-project\n"]}]},{"cell_type":"code","source":["# Note: If it doesn't work for you remember to add a shortcut to the Meili's shared folder to your drive as detailed here:\n","# https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab"],"metadata":{"id":"zKgQ_WHvxOE3","executionInfo":{"status":"ok","timestamp":1649011940457,"user_tz":240,"elapsed":3,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Now we are in the Github repository!\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0QpTnz_9AkF","executionInfo":{"status":"ok","timestamp":1649015001996,"user_tz":240,"elapsed":474,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"4665b611-2d83-4837-8f01-9b5db1db9dc6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'Deep Learning Final Project Data Visualization.ipynb'\n"," get_race_feature\n"," race_feature.csv\n"," README.md\n"," Scaling.ipynb\n"," train_face_detection.py\n"]}]},{"cell_type":"markdown","source":["# Github Integration Section (Only Need to Do Once!)"],"metadata":{"id":"lXev8ypP32M-"}},{"cell_type":"code","source":["# I followed this tutorial to get Github to integrate with Google Drive: \n","# https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d\n","\n","# GITHUB_TOKEN = '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'"],"metadata":{"id":"ywwgF_NN4Abf","executionInfo":{"status":"ok","timestamp":1649014499356,"user_tz":240,"elapsed":148,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Clone the repo into the \"CPSC 452 Deep Learning Final Project\" shared folder\n","# ONLY RUN ONCE: git clone https://{GITHUB_TOKEN}@github.com/ykim321/cpsc452-project.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGDzSvB64Ae-","executionInfo":{"status":"ok","timestamp":1649014522786,"user_tz":240,"elapsed":22619,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"557b3ae1-8d01-4a7a-8472-4e9ea753e338"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'cpsc452-project'...\n","remote: Enumerating objects: 139, done.\u001b[K\n","remote: Counting objects: 100% (139/139), done.\u001b[K\n","remote: Compressing objects: 100% (134/134), done.\u001b[K\n","remote: Total 139 (delta 5), reused 130 (delta 2), pack-reused 0\u001b[K\n","Receiving objects: 100% (139/139), 311.81 MiB | 15.38 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n"]}]},{"cell_type":"code","source":["# !git config user.email \"j.d.zhao@yale.edu\"\n","# !git config user.name \"James Zhao\""],"metadata":{"id":"xSU1l67R-gwu","executionInfo":{"status":"ok","timestamp":1649015447072,"user_tz":240,"elapsed":126,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# Git Operations for Pushing / Pulling / Etc (Can Do Multiple Times)"],"metadata":{"id":"QWz5PV6f7lN_"}},{"cell_type":"code","source":["# get repo status\n","!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rO82UQuu-D7h","executionInfo":{"status":"ok","timestamp":1649016449336,"user_tz":240,"elapsed":355,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"c2d579c9-3e8a-4bcb-a063-07e8f4cacd8c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\n","\t\u001b[31mDeep_Learning_Training.ipynb\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","source":["# pull latest repo data\n","!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEU9wfkUutUJ","executionInfo":{"status":"ok","timestamp":1649016455305,"user_tz":240,"elapsed":618,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"6b3303f4-3061-48eb-f7fb-acf570445b9f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"code","source":["# add updated files \n","!git add -u"],"metadata":{"id":"EBM6vqjd9c11","executionInfo":{"status":"ok","timestamp":1649016457822,"user_tz":240,"elapsed":315,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# commit changes to remote\n","!git commit -m \"Added ipynb notebook\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fblvr0jt9xnd","executionInfo":{"status":"ok","timestamp":1649016471463,"user_tz":240,"elapsed":193,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"1c18f4eb-3b2a-4de0-e579-efca99255660"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","\t\u001b[31mDeep_Learning_Training.ipynb\u001b[m\n","\n","nothing added to commit but untracked files present\n"]}]},{"cell_type":"code","source":["# push to remote\n","!git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mqAwG5Mx4Hj","executionInfo":{"status":"ok","timestamp":1649015475918,"user_tz":240,"elapsed":647,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"a8b1977a-4d88-43ea-8a8a-e17e5adf5411"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Everything up-to-date\n"]}]},{"cell_type":"code","source":["# This file contains the training data\n","def load_data_from_npz_file(file_path):\n","    \"\"\"\n","    Load data from npz file\n","    :param file_path: path to npz file with training data\n","    :return: input features and target data as numpy arrays\n","    \"\"\"\n","    data = np.load(file_path)\n","    return data['input'], data['target']\n","\n","# Mean and variance centering data\n","def normalize_data_per_row(data):\n","    \"\"\"\n","    Normalize a give matrix of data (samples must be organized per row)\n","    :param data: input data as a numpy array with dimensions NxHxWxC\n","    :return: normalized data with pixel values in [0,1] (array with same dimensions as input)\n","    \"\"\"\n","\n","    # sanity checks!\n","    assert len(data.shape) == 4, \"Expected the input data to be a 4D matrix\"\n","\n","    return data / 255\n","\n","# Split into train and test \n","def split_data(input, target, train_percent):\n","    \"\"\"\n","    Split the input and target data into two sets\n","    :param input: inputs [NxM] matrix\n","    :param target: target [Nx1] matrix\n","    :param train_percent: percentage of the data that should be assigned to training\n","    :return: train_input, train_target, test_input, test_target\n","    \"\"\"\n","    assert input.shape[0] == target.shape[0], \\\n","        \"Number of inputs and targets do not match ({} vs {})\".format(input.shape[0], target.shape[0])\n","\n","    indices = list(range(input.shape[0]))\n","    np.random.shuffle(indices)\n","\n","    num_train = int(input.shape[0]*train_percent)\n","    train_indices = indices[:num_train]\n","    val_indices = indices[num_train:]\n","\n","    return input[train_indices, :], target[train_indices,:], input[val_indices,:], target[val_indices,:]\n","\n","# Build the model\n","def build_nonlinear_model():\n","    \"\"\"\n","    Build NN model with Keras\n","    :param num_inputs: number of input features for the model\n","    :return: Keras model\n","    \"\"\"\n","    \n","    model = Sequential()\n","    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(64,64,3)))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation =\"relu\"))\n","    model.add(Flatten())\n","    model.add(Dense(32, activation=\"relu\"))\n","    model.add(Dense(1,activation=\"sigmoid\"))\n","    model.summary()\n","    \n","    return model\n","\n","def train_model(model, train_input, train_target, val_input, val_target,\n","                epochs=200, learning_rate=0.01, batch_size=16):\n","    \"\"\"\n","    Train the model on the given data\n","    :param model: Keras model\n","    :param train_input: train inputs\n","    :param train_target: train targets\n","    :param val_input: validation inputs\n","    :param val_target: validation targets\n","    :param input_mean: mean for the variables in the inputs (for normalization)\n","    :param input_stdev: st. dev. for the variables in the inputs (for normalization)\n","    :param epochs: epochs for gradient descent\n","    :param learning_rate: learning rate for gradient descent\n","    :param batch_size: batch size for training with gradient descent\n","    \"\"\"\n","\n","    norm_train_input = n2(train_input)\n","    norm_val_input = n2(val_input)\n","\n","    # compile the model: define optimizer, loss, and metrics\n","    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n","                 loss='binary_crossentropy',\n","                 metrics=['binary_accuracy'])\n","                 \n","     # tensorboard callback\n","    logs_dir = 'logs/log_{}'.format(datetime.datetime.now().strftime(\"%m-%d-%Y-%H-%M\"))\n","    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=logs_dir, write_graph=True)\n","\n","     # save checkpoint callback\n","    checkpointCallBack = tf.keras.callbacks.ModelCheckpoint(os.path.join(logs_dir,'best_weights.h5'),\n","                                                            monitor='binary_accuracy',\n","                                                            verbose=0,\n","                                                            save_best_only=True,\n","                                                            save_weights_only=False,\n","                                                            mode='auto',\n","                                                            save_freq=1)\n","\n","    # do training for the specified number of epochs and with the given batch size\n","    # TODO - Add callbacks to fit funciton\n","    model.fit(norm_train_input, train_target, epochs=epochs, batch_size=batch_size,\n","          validation_data=(norm_val_input, val_target),\n","          callbacks=[tbCallBack, checkpointCallBack])"],"metadata":{"id":"ezh2t1KMs8Zx","executionInfo":{"status":"ok","timestamp":1649010864657,"user_tz":240,"elapsed":357,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def main(npz_data_file, batch_size, epochs, lr, val, logs_dir, build_fn=build_nonlinear_model):\n","    \"\"\"\n","    Main function that performs training and test on a validation set\n","    :param npz_data_file: npz input file with training data\n","    :param batch_size: batch size to use at training time\n","    :param epochs: number of epochs to train for\n","    :param lr: learning rate\n","    :param val: percentage of the training data to use as validation\n","    :param logs_dir: directory where to save logs and trained parameters/weights\n","    \"\"\"\n","\n","    input, target = load_data_from_npz_file(npz_data_file)\n","    N = input.shape[0]\n","    assert N == target.shape[0], \\\n","        \"The input and target arrays had different amounts of data ({} vs {})\".format(N, target.shape[0]) # sanity check!\n","    print(\"Loaded {} training examples.\".format(N))\n","\n","    train_input, train_target, val_input, val_target = split_data(input, target, val)\n","    model = build_fn()\n","    train_model(model, train_input, train_target, val_input, val_target, epochs=epochs, learning_rate=lr, batch_size=batch_size)\n","\n","def n2(data):\n","    \"\"\"\n","    Normalize a give matrix of data (samples must be organized per row)\n","    :param data: input data as a numpy array with dimensions NxHxWxC\n","    :return: normalized data with pixel values in [0,1] (array with same dimensions as input)\n","    \"\"\"\n","\n","    # sanity checks!\n","    assert len(data.shape) == 4, \"Expected the input data to be a 4D matrix\"\n","\n","    if np.max(data) > 255:\n","        normalized_data = data / 255\n","    else:\n","        normalized_data = data\n","    return normalized_data\n"],"metadata":{"id":"ev0X1VK3tO9S","executionInfo":{"status":"ok","timestamp":1649010932786,"user_tz":240,"elapsed":195,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["main(\"./Data/64x64_data.npz\", 64, 100, 0.01, 0.1, \"./Data/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"Aq8R9Oq5tlFY","executionInfo":{"status":"error","timestamp":1649011140060,"user_tz":240,"elapsed":770,"user":{"displayName":"James Zhao","userId":"02904637952816276559"}},"outputId":"b24a9dfd-c64a-44af-a0b5-06c3a2562587"},"execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5ac4cc1bfd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Data/64x64_data.npz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./Data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-f030f9827fba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(npz_data_file, batch_size, epochs, lr, val, logs_dir, build_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_from_npz_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpz_data_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;34m\"The input and target arrays had different amounts of data ({} vs {})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sanity check!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-b624c214bd95>\u001b[0m in \u001b[0;36mload_data_from_npz_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/64x64_data.npz'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lpmlXLWJtYye"},"execution_count":null,"outputs":[]}]}